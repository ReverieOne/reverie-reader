{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Title: The Tensor Product, Demystified\n",
       "\n",
       "URL Source: https://www.math3ma.com/blog/the-tensor-product-demystified\n",
       "\n",
       "Markdown Content:\n",
       "Previously on the blog, we've discussed a recurring theme throughout mathematics: making new things from old things. Mathematicians do this all the time:\n",
       "\n",
       "*   When you have two integers, you can find their greatest common divisor or least common multiple.\n",
       "*   When you have some sets, you can form their Cartesian product or their union.\n",
       "*   When you have two groups, you can construct their direct sum or their free product.\n",
       "*   When you have a topological space, you can look for a subspace or a quotient space.\n",
       "*   When you have some vector spaces, you can ask for their direct sum or their intersection.\n",
       "*   The list goes on!\n",
       "\n",
       "Today, I'd like to focus on a particular way to build a new vector space from old vector spaces: _the tensor product_. This construction often come across as scary and mysterious, but I hope to help shine a little light and dispel some of the fear. In particular, we won't talk about axioms, universal properties, or commuting diagrams. Instead, we'll take an elementary, concrete look:\n",
       "\n",
       "Given two vectors $\\\\mathbf{v}$ and $\\\\mathbf{w}$, we can build a new vector, called the _tensor product_ $\\\\mathbf{v}\\\\otimes \\\\mathbf{w}$. But what is that vector, _really_? Likewise, given two vector spaces $V$ and $W$, we can build a new vector space, also called their _tensor product_ $V\\\\otimes W$. But what is that vector space, _really_?\n",
       "\n",
       "## Making new vectors from old\n",
       "\n",
       "In this discussion, we'll assume $V$ and $W$ are finite dimensional vector spaces. That means we can think of $V$ as $\\\\mathbb{R}^n$ and $W$ as $\\\\mathbb{R}^m$ for some positive integers $n$ and $m$. So a vector $\\\\mathbf{v}$ in $\\\\mathbb{R}^n$ is really just a list of $n$ numbers, while a vector $\\\\mathbf{w}$ in $\\\\mathbb{R}^m$ is just a list of $m$ numbers.\n",
       "\n",
       "Let's try to make new, third vector out of $\\\\mathbf{v}$ and $\\\\mathbf{w}$. But how? Here are two ideas: We can stack them on top of each other, or we can first multiply the numbers together and _then_ stack them on top of each other.\n",
       "\n",
       "The first option gives a new list of $n+m$ numbers, while the second option gives a new list of $nm$ numbers. The first gives a way to build a new space where the dimensions _add;_ the second gives a way to build a new space where the dimensions _multiply_. The first is a vector $(\\\\mathbf{v},\\\\mathbf{w})$ in the **direct sum** $V\\\\oplus W$ (this is the same as their direct product $V\\\\times W$); the second is a vector $\\\\mathbf{v}\\\\otimes \\\\mathbf{w}$ in the **tensor product** $V\\\\otimes W$.\n",
       "\n",
       "And that's it!\n",
       "\n",
       "Forming the tensor product $\\\\mathbf{v}\\\\otimes \\\\mathbf{w}$ of two vectors is _a lot_ like forming the Cartesian product of two sets $X\\\\times Y$. In fact, that's exactly what we're doing if we think of $X$ as the set whose elements are the entries of $\\\\mathbf{v}$ and similarly for $Y$.\n",
       "\n",
       "So a tensor product is like a grown-up version of multiplication. It's what happens when you systematically multiply a bunch of numbers together, then organize the results into a list. It's multi-multiplication, if you will.\n",
       "\n",
       "## There's a little more to the story.\n",
       "\n",
       "Does _every_ vector in $V\\\\otimes W$ look like $\\\\mathbf{v}\\\\otimes\\\\mathbf{w}$ for some $\\\\mathbf{v}\\\\in V$ and $\\\\mathbf{w}\\\\in W$? Not quite. Remember, a vector in a vector space can be written as a weighted sum of _basis vectors_, which are like the space's building blocks. This is another instance of making new things from existing ones: we get a new vector by taking a weighted sum of some special vectors!\n",
       "\n",
       "So a typical vector in $V\\\\otimes W$ is a weighted sum of basis vectors. W_hat are those basis vectors_? Well, there must be exactly $nm$ of them, since the dimension of $V\\\\otimes W$ is $nm$. Moreover, we'd expect them to be built up from the basis of $V$ _and_ the basis of $W$. This brings us again to the \"How can we construct new things from old things?\" question. Asked explicitly: If we have $n$ bases $\\\\mathbf{v}\\_1,\\\\ldots,\\\\mathbf{v}\\_n$ for $V$ and if we have $m$ bases $\\\\mathbf{w}\\_1,\\\\ldots,\\\\mathbf{w}\\_m$ for $W$ then how can we combine them to get a new set of $nm$ vectors?\n",
       "\n",
       "This is totally analogous to the construction we saw above: given a list of $n$ things and a list of $m$ things, we can obtain a list of $nm$ things by multiplying them all together. So we'll do the same thing here! We'll simply multiply the $\\\\mathbf{v}\\_i$ together with the $\\\\mathbf{w}\\_j$ in all possible combinations, _except_ \"multiply $\\\\mathbf{v}\\_i$ and $\\\\mathbf{w}\\_j$ \" now means \"take the tensor product of $\\\\mathbf{v}\\_i$ and $\\\\mathbf{w}\\_j$.\"\n",
       "\n",
       "Concretely, a basis for $V\\\\otimes W$ is the set of all vectors of the form $\\\\mathbf{v}\\_i\\\\otimes\\\\mathbf{w}\\_j$ where $i$ ranges from $1$ to $n$ and $j$ ranges from $1$ to $m$. As an example, suppose $n=3$ and $m=2$ as before. Then we can find the six basis vectors for $V\\\\otimes W$ by forming a 'multiplication chart.' (The sophisticated way to say this is: \"$V\\\\otimes W$ is the free vector space on $A\\\\times B$, where $A$ is a set of generators for $V$ and $B$ is a set of generators for $W$.\")\n",
       "\n",
       "So $V\\\\otimes W$ is the six-dimensional space with basis\n",
       "\n",
       "$$\\\\{\\\\mathbf{v}\\_1\\\\otimes\\\\mathbf{w}\\_1,\\\\;\\\\mathbf{v}\\_1\\\\otimes\\\\mathbf{w}\\_2,\\\\; \\\\mathbf{v}\\_2\\\\otimes\\\\mathbf{w}\\_1,\\\\;\\\\mathbf{v}\\_2\\\\otimes\\\\mathbf{w}\\_2,\\\\;\\\\mathbf{v}\\_3\\\\otimes\\\\mathbf{w}\\_1,\\\\;\\\\mathbf{v}\\_3\\\\otimes\\\\mathbf{w}\\_2 \\\\}$$\n",
       "\n",
       "This might feel a little abstract with all the $\\\\otimes$ symbols littered everywhere. But don't forget—we know exactly what each $\\\\mathbf{v}\\_i\\\\otimes\\\\mathbf{w}\\_j$ looks like—it's just a list of numbers! _Which list of numbers?_ Well,\n",
       "\n",
       "So what is $V\\\\otimes W$? It's the vector space whose vectors are linear combinations of the $\\\\mathbf{v}\\_i\\\\otimes\\\\mathbf{w}\\_j$. For example, here are a couple of vectors in this space:\n",
       "\n",
       "## Well, _technically_...\n",
       "\n",
       "Technically, $\\\\mathbf{v}\\\\otimes\\\\mathbf{w}$ is called the **outer product** of $\\\\mathbf{v}$ and $\\\\mathbf{w}$ and is defined by $$\\\\mathbf{v}\\\\otimes\\\\mathbf{w}:=\\\\mathbf{v}\\\\mathbf{w}^\\\\top$$ where $\\\\mathbf{w}^\\\\top$ is the same as $\\\\mathbf{w}$ but written as a row vector. (And if the entries of $\\\\mathbf{w}$ are complex numbers, then we also replace each entry by its complex conjugate.) So _technically_ the tensor product of vectors is matrix:\n",
       "\n",
       "This may seem to be in conflict with what we did above, but it's not! The two go hand-in-hand. Any $m\\\\times n$ matrix can be reshaped into a $nm\\\\times 1$ column vector and vice versa. (So thus far, we've exploiting the fact that $\\\\mathbb{R}^3\\\\otimes\\\\mathbb{R}^2$ is _isomorphic_ to $\\\\mathbb{R}^6$.) You might refer to this as _matrix-vector duality._\n",
       "\n",
       "It's a little like a **process-state duality**. On the one hand, a matrix $\\\\mathbf{v}\\\\otimes\\\\mathbf{w}$ is a _process_—it's a concrete representation of a (linear) _transformation._ On the other hand, $\\\\mathbf{v}\\\\otimes\\\\mathbf{w}$ is, abstractly speaking, a vector. And a _vector_ is the mathematical gadget that physicists use to describe the _state_ of a quantum system. So matrices encode processes; vectors encode states. The upshot is that a vector in a tensor product $V\\\\otimes W$ can be viewed in _either_ _way_ simply by reshaping the numbers as a list or as a rectangle.\n",
       "\n",
       "By the way, this idea of viewing a matrix as a process can easily be generalized to _higher dimensional arrays_, too. These arrays are called _tensors_ and whenever you do a bunch of _these_ processes together, the resulting mega-process gives rise to a **tensor network**. But manipulating high-dimensional arrays of numbers can get very messy very quickly: there are lots of numbers that _all_ have to be multiplied together. This is like multi-multi-multi-multi...plication. Fortunately, tensor networks come with lovely pictures that make these computations very simple. (It goes back to Roger Penrose's graphical calculus.) This is a conversation I'd like to have here, but it'll have to wait for another day!\n",
       "\n",
       "## In quantum physics\n",
       "\n",
       "One application of tensor products is related to the brief statement I made above: \"A _vector_ is the mathematical gadget that physicists use to describe the _state_ of a quantum system.\" To elaborate: if you have a little quantum particle, perhaps you’d like to know what it’s doing. Or what it’s capable of doing. Or the probability that it’ll be doing something. In essence, you're asking: What’s its status? What’s its _state_? The answer to this question— provided by a postulate of quantum mechanics—is given by a unit vector in a vector space. (Really, a Hilbert space, say $\\\\mathbb{C}^n$.) That unit vector encodes information about that particle.\n",
       "\n",
       "The dimension $n$ is, loosely speaking, the number of different things you could observe after making a measurement on the particle. But what if we have two little quantum particles? The state of that two-particle system can be described by something called a _density matrix_ $\\\\rho$ on the tensor product of their respective spaces $\\\\mathbb{C}^n\\\\otimes\\\\mathbb{C}^n$. A density matrix is a generalization of a unit vector—it accounts for interactions between the two particles.\n",
       "\n",
       "The same story holds for $N$ particles—the state of an $N$-particle system can be described by a density matrix on an $N$-fold tensor product.\n",
       "\n",
       "_But why the tensor product?_ Why is it that this construction—out of all things—describes the interactions within a quantum system so well, so naturally? I don’t know the answer, but perhaps the appropriateness of tensor products shouldn't be too surprising. The tensor product itself captures all ways that basic things can \"interact\" with each other!\n",
       "\n",
       "Of course, there's lots more to be said about tensor products. I've only shared a snippet of basic arithmetic. For a deeper look into the mathematics, I recommend reading through Jeremy Kun's wonderfully lucid How to Conquer Tensorphobia and Tensorphobia and the Outer Product. Enjoy!\n",
       "\n",
       "‍\n",
       "\n",
       "Images:\n",
       "- ![Image 1](https://cdn.prod.website-files.com/5b1d427ae0c922e912eda447/5bc29557faf7dc810b681e2d_vw.jpg)\n",
       "- ![Image 2](https://cdn.prod.website-files.com/5b1d427ae0c922e912eda447/5bf17a1638d56499ff9ff2ad_option%2012.jpg)\n",
       "- ![Image 3](https://cdn.prod.website-files.com/5b1d427ae0c922e912eda447/5bf17bd763157b3c9879bd06_analogy.jpg)\n",
       "- ![Image 4](https://cdn.prod.website-files.com/5b1d427ae0c922e912eda447/5bf17e62cb149a31764a85fa_XY.jpg)\n",
       "- ![Image 5](https://cdn.prod.website-files.com/5b1d427ae0c922e912eda447/5bf192dbd917b20e519b6304_examples.jpg)\n",
       "- ![Image 6](https://cdn.prod.website-files.com/5b1d427ae0c922e912eda447/5bf18109d730c7217fb4b1a3_basis.jpg)\n",
       "- ![Image 7](https://cdn.prod.website-files.com/5b1d427ae0c922e912eda447/5bc29013b7167189bc20f12e_tensor%20chart.jpg)\n",
       "- ![Image 8](https://cdn.prod.website-files.com/5b1d427ae0c922e912eda447/5bf1822ec4891ebb5d70a2ba_6bases.jpg)\n",
       "- ![Image 9](https://cdn.prod.website-files.com/5b1d427ae0c922e912eda447/5bf183c7d8afa54244f7b4ee_examples2.jpg)\n",
       "- ![Image 10](https://cdn.prod.website-files.com/5b1d427ae0c922e912eda447/5bc2961cdbf2a30ad3de11a3_outerprod.jpg)\n",
       "- ![Image 11](https://cdn.prod.website-files.com/5b1d427ae0c922e912eda447/5bc292a6ea24b52c94ed7392_reshape.jpg)\n",
       "- ![Image 12](https://cdn.prod.website-files.com/5b1d427ae0c922e912eda447/5bc2931fea24b51eb6ed7397_reshape2.jpg)\n",
       "- ![Image 13](https://cdn.prod.website-files.com/5b1d427ae0c922e912eda447/5bf18deabdfdc38254963c66_quantum1.jpg)\n",
       "- ![Image 14](https://cdn.prod.website-files.com/5b1d427ae0c922e912eda447/5bf18ccccb149a45764a8667_quantum2.jpg)\n",
       "- ![Image 15](https://cdn.prod.website-files.com/5b1d427ae0c922e912eda447/5bf18e7b1e03a4ec5d964fd8_quantum3.jpg)\n",
       "- ![Image 16](https://cdn.prod.website-files.com/5b1d427ae0c922e912eda447/5bf18a33dc7cd56fcae2ad0b_analogy0.jpg)\n",
       "- ![Image 17](https://cdn.prod.website-files.com/5b1d427ae0c922e912eda447/5b527d2844acede360b8e7ae_hline.jpg)\n",
       "\n",
       "Links/Buttons:\n",
       "- [Home](https://www.math3ma.com/)\n",
       "- [About](https://www.math3ma.com/about)\n",
       "- [categories](https://www.math3ma.com/categories)\n",
       "- [Subscribe](https://www.math3ma.com/subscribe)\n",
       "- [Institute](http://www.math3ma.institute/)\n",
       "- [shop](https://www.math3ma.com/blog/the-tensor-product-demystified#)\n",
       "- [](https://www.facebook.com/math3ma/)\n",
       "- [](https://twitter.com/math3ma)\n",
       "- [](https://www.instagram.com/math3ma/)\n",
       "- [Ps. 148](https://www.bible.com/bible/100/PSA.148.nasb)\n",
       "- [March 2023](https://www.math3ma.com/archive/march-2023)\n",
       "- [February 2023](https://www.math3ma.com/archive/february-2023)\n",
       "- [January 2023](https://www.math3ma.com/archive/january-2023)\n",
       "- [February 2022](https://www.math3ma.com/archive/february-2022)\n",
       "- [November 2021](https://www.math3ma.com/archive/november-2021)\n",
       "- [September 2021](https://www.math3ma.com/archive/september-2021)\n",
       "- [July 2021](https://www.math3ma.com/archive/july-2021)\n",
       "- [June 2021](https://www.math3ma.com/archive/june-2021)\n",
       "- [December 2020](https://www.math3ma.com/archive/december-2020)\n",
       "- [September 2020](https://www.math3ma.com/archive/september-2020)\n",
       "- [August 2020](https://www.math3ma.com/archive/august-2020)\n",
       "- [July 2020](https://www.math3ma.com/archive/july-2020)\n",
       "- [April 2020](https://www.math3ma.com/archive/april-2020)\n",
       "- [March 2020](https://www.math3ma.com/archive/march-2020)\n",
       "- [February 2020](https://www.math3ma.com/archive/february-2020)\n",
       "- [October 2019](https://www.math3ma.com/archive/october-2019)\n",
       "- [September 2019](https://www.math3ma.com/archive/september-2019)\n",
       "- [July 2019](https://www.math3ma.com/archive/july-2019)\n",
       "- [May 2019](https://www.math3ma.com/archive/may-2019)\n",
       "- [March 2019](https://www.math3ma.com/archive/march-2019)\n",
       "- [January 2019](https://www.math3ma.com/archive/january-2019)\n",
       "- [November 18, 2018](https://www.math3ma.com/archive/november-2018)\n",
       "- [October 2018](https://www.math3ma.com/archive/october-2018)\n",
       "- [September 2018](https://www.math3ma.com/archive/september-2018)\n",
       "- [May 2018](https://www.math3ma.com/archive/march-2018)\n",
       "- [February 2018](https://www.math3ma.com/archive/february-2018)\n",
       "- [January 2018](https://www.math3ma.com/archive/january-2018)\n",
       "- [December 2017](https://www.math3ma.com/archive/december-2017)\n",
       "- [November 2017](https://www.math3ma.com/archive/november-2017)\n",
       "- [October 2017](https://www.math3ma.com/archive/october-2017)\n",
       "- [September 2017](https://www.math3ma.com/archive/september-2017)\n",
       "- [August 2017](https://www.math3ma.com/archive/august-2017)\n",
       "- [July 2017](https://www.math3ma.com/archive/july-2017)\n",
       "- [June 2017](https://www.math3ma.com/archive/june-2017)\n",
       "- [May 2017](https://www.math3ma.com/archive/may-2017)\n",
       "- [April 2017](https://www.math3ma.com/archive/april-2017)\n",
       "- [March 2017](https://www.math3ma.com/archive/march-2017)\n",
       "- [February 2017](https://www.math3ma.com/archive/february-2017)\n",
       "- [January 2017](https://www.math3ma.com/archive/january-2017)\n",
       "- [December 2016](https://www.math3ma.com/archive/december-2016)\n",
       "- [November 2016](https://www.math3ma.com/archive/november-2016)\n",
       "- [October 2016](https://www.math3ma.com/archive/october-2016)\n",
       "- [September 2016](https://www.math3ma.com/archive/september-2016)\n",
       "- [August 2016](https://www.math3ma.com/archive/august-2016)\n",
       "- [July 2016](https://www.math3ma.com/archive/july-2016)\n",
       "- [June 2016](https://www.math3ma.com/archive/june-2016)\n",
       "- [May 2016](https://www.math3ma.com/archive/may-2016)\n",
       "- [April 2016](https://www.math3ma.com/archive/april-2016)\n",
       "- [March 2016](https://www.math3ma.com/archive/march-2016)\n",
       "- [February 2016](https://www.math3ma.com/archive/february-2016)\n",
       "- [January 2016](https://www.math3ma.com/archive/january-2016)\n",
       "- [December 2015](https://www.math3ma.com/archive/december-2015)\n",
       "- [November 2015](https://www.math3ma.com/archive/november-2015)\n",
       "- [October 2015](https://www.math3ma.com/archive/october-2015)\n",
       "- [September 2015](https://www.math3ma.com/archive/september-2015)\n",
       "- [August 2015](https://www.math3ma.com/archive/august-2015)\n",
       "- [July 2015](https://www.math3ma.com/archive/july-2015)\n",
       "- [June 2015](https://www.math3ma.com/archive/june-2015)\n",
       "- [May 2015](https://www.math3ma.com/archive/may-2015)\n",
       "- [April 2015](https://www.math3ma.com/archive/april-2015)\n",
       "- [March 2015](https://www.math3ma.com/archive/march-2015)\n",
       "- [February 2015](https://www.math3ma.com/archive/february-2015)\n",
       "- [Algebra](https://www.math3ma.com/categories/algebra)\n",
       "- [Previously on the blog](https://www.math3ma.com/blog/limits-and-colimits-part-1)\n",
       "- [Commutative Diagrams Explained](https://www.math3ma.com/blog/commutative-diagrams-explained)\n",
       "- [Roger Penrose's graphical calculus](https://en.wikipedia.org/wiki/Penrose_graphical_notation)\n",
       "- [another day](https://www.math3ma.com/blog/matrices-as-tensor-network-diagrams)\n",
       "- [How to Conquer Tensorphobia](https://jeremykun.com/2014/01/17/how-to-conquer-tensorphobia/)\n",
       "- [Tensorphobia and the Outer Product](https://jeremykun.com/2016/03/28/tensorphobia-outer-product/)\n",
       "- [Tweet](https://twitter.com/share?ref_src=twsrc%5Etfw)\n",
       "- [Share](https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fdevelopers.facebook.com%2Fdocs%2Fplugins%2F&src=sdkpreparse)\n",
       "- [Ways to Show a Group is Abelian](https://www.math3ma.com/blog/ways-to-show-a-group-is-abelian)\n",
       "- [Constructing the Tensor Product of Modules](https://www.math3ma.com/blog/constructing-the-tensor-product-of-modules)\n",
       "- [What is Galois Theory Anyway?](https://www.math3ma.com/blog/what-is-galois-theory-anyway)\n",
       "- [comments powered by Disqus.](https://disqus.com/?ref_noscript)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import requests\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "base_url = 'http://localhost:8072'\n",
    "# base_url = 'https://r.jina.ai'\n",
    "\n",
    "url_to_scrape = 'https://www.math3ma.com/blog/the-tensor-product-demystified'\n",
    "\n",
    "remove_images = True\n",
    "remove_links = True\n",
    "\n",
    "url = f'{base_url}/{url_to_scrape}'\n",
    "headers = {\n",
    "    'X-With-Images-Summary': 'true',\n",
    "    'X-With-Links-Summary': 'true',\n",
    "    'X-Remove-Images-From-Markdown': 'true',\n",
    "    'X-Remove-Links-From-Markdown': 'true',\n",
    "}\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "display(Markdown(response.text))\n",
    "# print(response.json())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
